{
    "sourceFile": "src/recognition/postprocessor.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 23,
            "patches": [
                {
                    "date": 1764291606882,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1764302990061,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,32 +1,53 @@\n from __future__ import annotations\r\n \r\n import re\r\n from dataclasses import dataclass\r\n+from typing import Any, Dict\r\n \r\n+import httpx  # type: ignore[import]\r\n+from loguru import logger  # type: ignore[import]\r\n+\r\n from config.settings import PostprocessConfig\r\n \r\n \r\n @dataclass\r\n class TextPostprocessor:\r\n     \"\"\"\r\n-    MVP-реализация постобработки текста.\r\n+    Постобработка текста.\r\n \r\n-    Сейчас:\r\n-      - если disabled в конфиге — возвращает текст как есть\r\n-      - если mode == \"simple\" или \"llm\" — применяет только простые regex-правки\r\n-    Позже сюда можно добавить вызовы LLM (Groq/OpenAI) по полям из PostprocessConfig.\r\n+    Режимы:\r\n+      - enabled = False  -> только возвращаем текст как есть (без даже regex)\r\n+      - enabled = True, mode = \"simple\" -> лёгкая regex-очистка\r\n+      - enabled = True, mode = \"llm\"    -> regex-очистка + LLM (Groq/OpenAI)\r\n+\r\n+    ВАЖНО: при любой ошибке LLM мы не ломаем UX, а возвращаем regex-вариант.\r\n     \"\"\"\r\n \r\n     config: PostprocessConfig\r\n \r\n     def process(self, text: str) -> str:\r\n+        text = text or \"\"\r\n+\r\n+        # Постпроцессинг полностью выключен\r\n         if not self.config.enabled:\r\n-            return text or \"\"\r\n+            return self._simple_cleanup(text)\r\n \r\n-        # На первом этапе не различаем simple/llm — делаем только лёгкую очистку\r\n-        return self._simple_cleanup(text or \"\")\r\n+        # Только простая очистка\r\n+        if (self.config.mode or \"simple\").lower() == \"simple\":\r\n+            return self._simple_cleanup(text)\r\n \r\n+        # Режим LLM: сначала regex, потом попытка прогнать через модель\r\n+        cleaned = self._simple_cleanup(text)\r\n+        try:\r\n+            llm_text = self._llm_cleanup(cleaned)\r\n+            return llm_text or cleaned\r\n+        except Exception as exc:  # noqa: BLE001\r\n+            logger.exception(\"LLM postprocess failed, fallback to regex-only: {}\", exc)\r\n+            return cleaned\r\n+\r\n+    # ------------------------------------------------------------------ simple regex\r\n+\r\n     @staticmethod\r\n     def _simple_cleanup(text: str) -> str:\r\n         # Удалить лишние пробелы\r\n         text = re.sub(r\"\\s+\", \" \", text).strip()\r\n@@ -39,5 +60,151 @@\n \r\n         # Удалить артефакты вида [BLANK_AUDIO] и т.п.\r\n         text = re.sub(r\"\\[[^\\]]+\\]\", \"\", text).strip()\r\n \r\n-        return text\n\\ No newline at end of file\n+        return text\r\n+\r\n+    # ------------------------------------------------------------------ LLM cleanup\r\n+\r\n+    def _llm_cleanup(self, text: str) -> str:\r\n+        \"\"\"\r\n+        Прогоняет текст через LLM (Groq/OpenAI) для улучшения грамматики и пунктуации.\r\n+\r\n+        Промпт:\r\n+        «Исправь опечатки, добавь пунктуацию, сделай предложение грамматически верным.\r\n+         Не меняй смысл. Ответь ТОЛЬКО исправленным текстом.»\r\n+        \"\"\"\r\n+        backend = (self.config.llm_backend or \"groq\").lower()\r\n+\r\n+        if backend == \"groq\":\r\n+            return self._llm_groq(text)\r\n+        if backend == \"openai\":\r\n+            return self._llm_openai(text)\r\n+\r\n+        logger.warning(\"Unknown LLM backend '{}', fallback to original text\", backend)\r\n+        return text\r\n+\r\n+    def _llm_groq(self, text: str) -> str:\r\n+        api_key = (self.config.groq.api_key or \"\").strip()\r\n+        model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n+\r\n+        if not api_key:\r\n+            raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n+\r\n+        url = \"https://api.groq.com/openai/v1/chat/completions\"\r\n+        headers = {\r\n+            \"Authorization\": f\"Bearer {api_key}\",\r\n+            \"Content-Type\": \"application/json\",\r\n+        }\r\n+        payload: Dict[str, Any] = {\r\n+            \"model\": model,\r\n+            \"messages\": [\r\n+                {\r\n+                    \"role\": \"system\",\r\n+                    \"content\": (\r\n+                        \"Ты помощник по русскому языку. \"\r\n+                        \"Исправь опечатки, добавь пунктуацию, сделай текст грамматически верным. \"\r\n+                        \"Не меняй смысл. Ответь ТОЛЬКО исправленным текстом без пояснений.\"\r\n+                    ),\r\n+                },\r\n+                {\"role\": \"user\", \"content\": text},\r\n+            ],\r\n+            \"temperature\": 0.0,\r\n+        }\r\n+\r\n+        try:\r\n+            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n+        except httpx.TimeoutException as exc:\r\n+            logger.exception(\"Groq LLM timeout: {}\", exc)\r\n+            raise RuntimeError(\"Groq LLM: превышено время ожидания ответа.\") from exc\r\n+        except httpx.RequestError as exc:\r\n+            logger.exception(\"Groq LLM network error: {}\", exc)\r\n+            raise RuntimeError(\"Groq LLM: сетевая ошибка при обращении к API.\") from exc\r\n+\r\n+        if resp.status_code == 401:\r\n+            raise RuntimeError(\"Groq LLM: неверный или отсутствующий API‑ключ (401).\")\r\n+        if resp.status_code == 429:\r\n+            raise RuntimeError(\"Groq LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n+        if not resp.is_success:\r\n+            logger.error(\"Groq LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n+            raise RuntimeError(f\"Groq LLM: ошибка сервера ({resp.status_code}).\")\r\n+\r\n+        try:\r\n+            data = resp.json()\r\n+        except ValueError as exc:\r\n+            logger.exception(\"Groq LLM JSON parse error: {}\", exc)\r\n+            raise RuntimeError(\"Groq LLM: не удалось разобрать ответ сервера.\") from exc\r\n+\r\n+        try:\r\n+            content = data[\"choices\"][0][\"message\"][\"content\"]\r\n+        except Exception as exc:  # noqa: BLE001\r\n+            logger.error(\"Groq LLM unexpected response format: {}\", data)\r\n+            raise RuntimeError(\"Groq LLM: неожиданный формат ответа.\") from exc\r\n+\r\n+        if not isinstance(content, str):\r\n+            raise RuntimeError(\"Groq LLM: контент ответа не является строкой.\")\r\n+\r\n+        return content.strip()\r\n+\r\n+    def _llm_openai(self, text: str) -> str:\r\n+        api_key = (self.config.openai.api_key or \"\").strip()\r\n+        model = (self.config.openai.model or \"gpt-5.1\").strip()\r\n+        base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n+\r\n+        if not api_key:\r\n+            raise RuntimeError(\"OpenAI LLM: API‑ключ не задан.\")\r\n+\r\n+        # Совместимый с OpenAI / proxy формат /chat/completions\r\n+        url = base_url.rstrip(\"/\") + \"/chat/completions\"\r\n+        headers = {\r\n+            \"Authorization\": f\"Bearer {api_key}\",\r\n+            \"Content-Type\": \"application/json\",\r\n+        }\r\n+        payload: Dict[str, Any] = {\r\n+            \"model\": model,\r\n+            \"messages\": [\r\n+                {\r\n+                    \"role\": \"system\",\r\n+                    \"content\": (\r\n+                        \"Ты помощник по русскому языку. \"\r\n+                        \"Исправь опечатки, добавь пунктуацию, сделай текст грамматически верным. \"\r\n+                        \"Не меняй смысл. Ответь ТОЛЬКО исправленным текстом без пояснений.\"\r\n+                    ),\r\n+                },\r\n+                {\"role\": \"user\", \"content\": text},\r\n+            ],\r\n+            \"temperature\": 0.0,\r\n+        }\r\n+\r\n+        try:\r\n+            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n+        except httpx.TimeoutException as exc:\r\n+            logger.exception(\"OpenAI LLM timeout: {}\", exc)\r\n+            raise RuntimeError(\"OpenAI LLM: превышено время ожидания ответа.\") from exc\r\n+        except httpx.RequestError as exc:\r\n+            logger.exception(\"OpenAI LLM network error: {}\", exc)\r\n+            raise RuntimeError(\"OpenAI LLM: сетевая ошибка при обращении к API.\") from exc\r\n+\r\n+        if resp.status_code == 401:\r\n+            raise RuntimeError(\"OpenAI LLM: неверный или отсутствующий API‑ключ (401).\")\r\n+        if resp.status_code == 429:\r\n+            raise RuntimeError(\"OpenAI LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n+        if not resp.is_success:\r\n+            logger.error(\"OpenAI LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n+            raise RuntimeError(f\"OpenAI LLM: ошибка сервера ({resp.status_code}).\")\r\n+\r\n+        try:\r\n+            data = resp.json()\r\n+        except ValueError as exc:\r\n+            logger.exception(\"OpenAI LLM JSON parse error: {}\", exc)\r\n+            raise RuntimeError(\"OpenAI LLM: не удалось разобрать ответ сервера.\") from exc\r\n+\r\n+        try:\r\n+            content = data[\"choices\"][0][\"message\"][\"content\"]\r\n+        except Exception as exc:  # noqa: BLE001\r\n+            logger.error(\"OpenAI LLM unexpected response format: {}\", data)\r\n+            raise RuntimeError(\"OpenAI LLM: неожиданный формат ответа.\") from exc\r\n+\r\n+        if not isinstance(content, str):\r\n+            raise RuntimeError(\"OpenAI LLM: контент ответа не является строкой.\")\r\n+\r\n+        return content.strip()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1764303523225,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -83,8 +83,9 @@\n         logger.warning(\"Unknown LLM backend '{}', fallback to original text\", backend)\r\n         return text\r\n \r\n     def _llm_groq(self, text: str) -> str:\r\n+        # Берём ключ в приоритете из groq.api_key (первое место в конфиге/настройках)\r\n         api_key = (self.config.groq.api_key or \"\").strip()\r\n         model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n \r\n         if not api_key:\r\n"
                },
                {
                    "date": 1764303637758,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,223 @@\n+from __future__ import annotations\r\n+\r\n+import re\r\n+from dataclasses import dataclass\r\n+from typing import Any, Dict\r\n+\r\n+import httpx  # type: ignore[import]\r\n+from loguru import logger  # type: ignore[import]\r\n+\r\n+from config.settings import PostprocessConfig\r\n+\r\n+\r\n+@dataclass\r\n+class TextPostprocessor:\r\n+    \"\"\"\r\n+    Постобработка текста.\r\n+\r\n+    Режимы:\r\n+      - enabled = False  -> только возвращаем текст как есть (без даже regex)\r\n+      - enabled = True, mode = \"simple\" -> лёгкая regex-очистка\r\n+      - enabled = True, mode = \"llm\"    -> regex-очистка + LLM (Groq/OpenAI)\r\n+\r\n+    ВАЖНО: при любой ошибке LLM мы не ломаем UX, а возвращаем regex-вариант.\r\n+    \"\"\"\r\n+\r\n+    config: PostprocessConfig\r\n+\r\n+    def process(self, text: str) -> str:\r\n+        text = text or \"\"\r\n+\r\n+        # Постпроцессинг полностью выключен\r\n+        if not self.config.enabled:\r\n+            return self._simple_cleanup(text)\r\n+\r\n+        # Только простая очистка\r\n+        if (self.config.mode or \"simple\").lower() == \"simple\":\r\n+            return self._simple_cleanup(text)\r\n+\r\n+        # Режим LLM: сначала regex, потом попытка прогнать через модель.\r\n+        # Если ключей нет или что-то падает — тихо откатываемся к regex,\r\n+        # без выброса исключения наружу (чтобы не ломать поток записи).\r\n+        cleaned = self._simple_cleanup(text)\r\n+\r\n+        # Если нет ключа для выбранного backend'а — сразу возвращаем regex.\r\n+        backend = (self.config.llm_backend or \"groq\").lower()\r\n+        if backend == \"groq\" and not (self.config.groq.api_key or \"\").strip():\r\n+            logger.warning(\"Groq LLM postprocess skipped: API key is empty\")\r\n+            return cleaned\r\n+        if backend == \"openai\" and not (self.config.openai.api_key or \"\").strip():\r\n+            logger.warning(\"OpenAI LLM postprocess skipped: API key is empty\")\r\n+            return cleaned\r\n+\r\n+        try:\r\n+            llm_text = self._llm_cleanup(cleaned)\r\n+            return llm_text or cleaned\r\n+        except Exception as exc:  # noqa: BLE001\r\n+            logger.exception(\"LLM postprocess failed, fallback to regex-only: {}\", exc)\r\n+            return cleaned\r\n+\r\n+    # ------------------------------------------------------------------ simple regex\r\n+\r\n+    @staticmethod\r\n+    def _simple_cleanup(text: str) -> str:\r\n+        # Удалить лишние пробелы\r\n+        text = re.sub(r\"\\s+\", \" \", text).strip()\r\n+\r\n+        # Пробел перед знаками препинания убрать\r\n+        text = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", text)\r\n+\r\n+        # Пробел после запятой/точки/восклицательного/вопросительного знака\r\n+        text = re.sub(r\"([,.!?;:])([^\\s])\", r\"\\1 \\2\", text)\r\n+\r\n+        # Удалить артефакты вида [BLANK_AUDIO] и т.п.\r\n+        text = re.sub(r\"\\[[^\\]]+\\]\", \"\", text).strip()\r\n+\r\n+        return text\r\n+\r\n+    # ------------------------------------------------------------------ LLM cleanup\r\n+\r\n+    def _llm_cleanup(self, text: str) -> str:\r\n+        \"\"\"\r\n+        Прогоняет текст через LLM (Groq/OpenAI) для улучшения грамматики и пунктуации.\r\n+\r\n+        Промпт:\r\n+        «Исправь опечатки, добавь пунктуацию, сделай предложение грамматически верным.\r\n+         Не меняй смысл. Ответь ТОЛЬКО исправленным текстом.»\r\n+        \"\"\"\r\n+        backend = (self.config.llm_backend or \"groq\").lower()\r\n+\r\n+        if backend == \"groq\":\r\n+            return self._llm_groq(text)\r\n+        if backend == \"openai\":\r\n+            return self._llm_openai(text)\r\n+\r\n+        logger.warning(\"Unknown LLM backend '{}', fallback to original text\", backend)\r\n+        return text\r\n+\r\n+    def _llm_groq(self, text: str) -> str:\r\n+        # Берём ключ в приоритете из groq.api_key (первое место в конфиге/настройках)\r\n+        api_key = (self.config.groq.api_key or \"\").strip()\r\n+        model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n+\r\n+        if not api_key:\r\n+            raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n+\r\n+        url = \"https://api.groq.com/openai/v1/chat/completions\"\r\n+        headers = {\r\n+            \"Authorization\": f\"Bearer {api_key}\",\r\n+            \"Content-Type\": \"application/json\",\r\n+        }\r\n+        payload: Dict[str, Any] = {\r\n+            \"model\": model,\r\n+            \"messages\": [\r\n+                {\r\n+                    \"role\": \"system\",\r\n+                    \"content\": (\r\n+                        \"Ты помощник по русскому языку. \"\r\n+                        \"Исправь опечатки, добавь пунктуацию, сделай текст грамматически верным. \"\r\n+                        \"Не меняй смысл. Ответь ТОЛЬКО исправленным текстом без пояснений.\"\r\n+                    ),\r\n+                },\r\n+                {\"role\": \"user\", \"content\": text},\r\n+            ],\r\n+            \"temperature\": 0.0,\r\n+        }\r\n+\r\n+        try:\r\n+            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n+        except httpx.TimeoutException as exc:\r\n+            logger.exception(\"Groq LLM timeout: {}\", exc)\r\n+            raise RuntimeError(\"Groq LLM: превышено время ожидания ответа.\") from exc\r\n+        except httpx.RequestError as exc:\r\n+            logger.exception(\"Groq LLM network error: {}\", exc)\r\n+            raise RuntimeError(\"Groq LLM: сетевая ошибка при обращении к API.\") from exc\r\n+\r\n+        if resp.status_code == 401:\r\n+            raise RuntimeError(\"Groq LLM: неверный или отсутствующий API‑ключ (401).\")\r\n+        if resp.status_code == 429:\r\n+            raise RuntimeError(\"Groq LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n+        if not resp.is_success:\r\n+            logger.error(\"Groq LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n+            raise RuntimeError(f\"Groq LLM: ошибка сервера ({resp.status_code}).\")\r\n+\r\n+        try:\r\n+            data = resp.json()\r\n+        except ValueError as exc:\r\n+            logger.exception(\"Groq LLM JSON parse error: {}\", exc)\r\n+            raise RuntimeError(\"Groq LLM: не удалось разобрать ответ сервера.\") from exc\r\n+\r\n+        try:\r\n+            content = data[\"choices\"][0][\"message\"][\"content\"]\r\n+        except Exception as exc:  # noqa: BLE001\r\n+            logger.error(\"Groq LLM unexpected response format: {}\", data)\r\n+            raise RuntimeError(\"Groq LLM: неожиданный формат ответа.\") from exc\r\n+\r\n+        if not isinstance(content, str):\r\n+            raise RuntimeError(\"Groq LLM: контент ответа не является строкой.\")\r\n+\r\n+        return content.strip()\r\n+\r\n+    def _llm_openai(self, text: str) -> str:\r\n+        api_key = (self.config.openai.api_key or \"\").strip()\r\n+        model = (self.config.openai.model or \"gpt-5.1\").strip()\r\n+        base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n+\r\n+        if not api_key:\r\n+            raise RuntimeError(\"OpenAI LLM: API‑ключ не задан.\")\r\n+\r\n+        # Совместимый с OpenAI / proxy формат /chat/completions\r\n+        url = base_url.rstrip(\"/\") + \"/chat/completions\"\r\n+        headers = {\r\n+            \"Authorization\": f\"Bearer {api_key}\",\r\n+            \"Content-Type\": \"application/json\",\r\n+        }\r\n+        payload: Dict[str, Any] = {\r\n+            \"model\": model,\r\n+            \"messages\": [\r\n+                {\r\n+                    \"role\": \"system\",\r\n+                    \"content\": (\r\n+                        \"Ты помощник по русскому языку. \"\r\n+                        \"Исправь опечатки, добавь пунктуацию, сделай текст грамматически верным. \"\r\n+                        \"Не меняй смысл. Ответь ТОЛЬКО исправленным текстом без пояснений.\"\r\n+                    ),\r\n+                },\r\n+                {\"role\": \"user\", \"content\": text},\r\n+            ],\r\n+            \"temperature\": 0.0,\r\n+        }\r\n+\r\n+        try:\r\n+            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n+        except httpx.TimeoutException as exc:\r\n+            logger.exception(\"OpenAI LLM timeout: {}\", exc)\r\n+            raise RuntimeError(\"OpenAI LLM: превышено время ожидания ответа.\") from exc\r\n+        except httpx.RequestError as exc:\r\n+            logger.exception(\"OpenAI LLM network error: {}\", exc)\r\n+            raise RuntimeError(\"OpenAI LLM: сетевая ошибка при обращении к API.\") from exc\r\n+\r\n+        if resp.status_code == 401:\r\n+            raise RuntimeError(\"OpenAI LLM: неверный или отсутствующий API‑ключ (401).\")\r\n+        if resp.status_code == 429:\r\n+            raise RuntimeError(\"OpenAI LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n+        if not resp.is_success:\r\n+            logger.error(\"OpenAI LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n+            raise RuntimeError(f\"OpenAI LLM: ошибка сервера ({resp.status_code}).\")\r\n+\r\n+        try:\r\n+            data = resp.json()\r\n+        except ValueError as exc:\r\n+            logger.exception(\"OpenAI LLM JSON parse error: {}\", exc)\r\n+            raise RuntimeError(\"OpenAI LLM: не удалось разобрать ответ сервера.\") from exc\r\n+\r\n+        try:\r\n+            content = data[\"choices\"][0][\"message\"][\"content\"]\r\n+        except Exception as exc:  # noqa: BLE001\r\n+            logger.error(\"OpenAI LLM unexpected response format: {}\", data)\r\n+            raise RuntimeError(\"OpenAI LLM: неожиданный формат ответа.\") from exc\r\n+\r\n+        if not isinstance(content, str):\r\n+            raise RuntimeError(\"OpenAI LLM: контент ответа не является строкой.\")\r\n+\r\n+        return content.strip()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1764303758708,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -95,9 +95,14 @@\n         logger.warning(\"Unknown LLM backend '{}', fallback to original text\", backend)\r\n         return text\r\n \r\n     def _llm_groq(self, text: str) -> str:\r\n-        # Берём ключ в приоритете из groq.api_key (первое место в конфиге/настройках)\r\n+        \"\"\"\r\n+        Для препроцессинга используем тот же ключ, что и для Whisper через Groq,\r\n+        чтобы пользователь вводил его один раз в одном месте.\r\n+        \"\"\"\r\n+        # Ключ берём из recognition.groq.api_key, который уже используется для Whisper.\r\n+        # Он прокидывается сюда через PostprocessConfig.groq.api_key при сохранении настроек.\r\n         api_key = (self.config.groq.api_key or \"\").strip()\r\n         model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n \r\n         if not api_key:\r\n"
                },
                {
                    "date": 1764304372942,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,11 +42,11 @@\n         cleaned = self._simple_cleanup(text)\r\n \r\n         # Если нет ключа для выбранного backend'а — сразу возвращаем regex.\r\n         backend = (self.config.llm_backend or \"groq\").lower()\r\n-        if backend == \"groq\" and not (self.config.groq.api_key or \"\").strip():\r\n-            logger.warning(\"Groq LLM postprocess skipped: API key is empty\")\r\n-            return cleaned\r\n+        # Для Groq LLM больше НЕ проверяем отдельный ключ: используется тот же,\r\n+        # что и для распознавания (recognition.groq.api_key), который прокидывается\r\n+        # в TextPostprocessor извне. Здесь просто даём LLM самому отработать.\r\n         if backend == \"openai\" and not (self.config.openai.api_key or \"\").strip():\r\n             logger.warning(\"OpenAI LLM postprocess skipped: API key is empty\")\r\n             return cleaned\r\n \r\n@@ -98,11 +98,13 @@\n     def _llm_groq(self, text: str) -> str:\r\n         \"\"\"\r\n         Для препроцессинга используем тот же ключ, что и для Whisper через Groq,\r\n         чтобы пользователь вводил его один раз в одном месте.\r\n+\r\n+        ВАЖНО:\r\n+        - отдельного postprocess.groq.api_key больше нет;\r\n+        - сюда должен быть уже прокинут рабочий ключ из recognition.groq.api_key.\r\n         \"\"\"\r\n-        # Ключ берём из recognition.groq.api_key, который уже используется для Whisper.\r\n-        # Он прокидывается сюда через PostprocessConfig.groq.api_key при сохранении настроек.\r\n         api_key = (self.config.groq.api_key or \"\").strip()\r\n         model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n \r\n         if not api_key:\r\n"
                },
                {
                    "date": 1764305028364,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,11 +42,17 @@\n         cleaned = self._simple_cleanup(text)\r\n \r\n         # Если нет ключа для выбранного backend'а — сразу возвращаем regex.\r\n         backend = (self.config.llm_backend or \"groq\").lower()\r\n-        # Для Groq LLM больше НЕ проверяем отдельный ключ: используется тот же,\r\n-        # что и для распознавания (recognition.groq.api_key), который прокидывается\r\n-        # в TextPostprocessor извне. Здесь просто даём LLM самому отработать.\r\n+        # Для Groq: ключ берём из recognition.groq.api_key, который должен быть\r\n+        # прокинут сюда заранее (в PostprocessConfig.groq.api_key). Если его нет —\r\n+        # не пытаемся вызывать LLM, просто возвращаем regex-вариант.\r\n+        if backend == \"groq\":\r\n+            api_key = getattr(self.config.groq, \"api_key\", \"\") or \"\"\r\n+            if not api_key.strip():\r\n+                logger.warning(\"Groq LLM postprocess skipped: API key is empty\")\r\n+                return cleaned\r\n+\r\n         if backend == \"openai\" and not (self.config.openai.api_key or \"\").strip():\r\n             logger.warning(\"OpenAI LLM postprocess skipped: API key is empty\")\r\n             return cleaned\r\n \r\n@@ -100,12 +106,12 @@\n         Для препроцессинга используем тот же ключ, что и для Whisper через Groq,\r\n         чтобы пользователь вводил его один раз в одном месте.\r\n \r\n         ВАЖНО:\r\n-        - отдельного postprocess.groq.api_key больше нет;\r\n-        - сюда должен быть уже прокинут рабочий ключ из recognition.groq.api_key.\r\n+        - если у GroqPostprocessConfig нет поля api_key, берём его через getattr;\r\n+        - ожидается, что туда заранее прокинут recognition.groq.api_key.\r\n         \"\"\"\r\n-        api_key = (self.config.groq.api_key or \"\").strip()\r\n+        api_key = (getattr(self.config.groq, \"api_key\", \"\") or \"\").strip()\r\n         model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n"
                },
                {
                    "date": 1764305445229,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,20 +42,24 @@\n         cleaned = self._simple_cleanup(text)\r\n \r\n         # Если нет ключа для выбранного backend'а — сразу возвращаем regex.\r\n         backend = (self.config.llm_backend or \"groq\").lower()\r\n-        # Для Groq: ключ берём из recognition.groq.api_key, который должен быть\r\n-        # прокинут сюда заранее (в PostprocessConfig.groq.api_key). Если его нет —\r\n-        # не пытаемся вызывать LLM, просто возвращаем regex-вариант.\r\n+\r\n+        # Для Groq/OpenAI больше НЕТ отдельных ключей в блоке postprocess.\r\n+        # Используем только recognition.*.api_key, который должен быть\r\n+        # передан сюда извне (например, через App). Если его нет — просто\r\n+        # не вызываем LLM и возвращаем regex-вариант.\r\n         if backend == \"groq\":\r\n             api_key = getattr(self.config.groq, \"api_key\", \"\") or \"\"\r\n             if not api_key.strip():\r\n                 logger.warning(\"Groq LLM postprocess skipped: API key is empty\")\r\n                 return cleaned\r\n \r\n-        if backend == \"openai\" and not (self.config.openai.api_key or \"\").strip():\r\n-            logger.warning(\"OpenAI LLM postprocess skipped: API key is empty\")\r\n-            return cleaned\r\n+        if backend == \"openai\":\r\n+            api_key = getattr(self.config.openai, \"api_key\", \"\") or \"\"\r\n+            if not api_key.strip():\r\n+                logger.warning(\"OpenAI LLM postprocess skipped: API key is empty\")\r\n+                return cleaned\r\n \r\n         try:\r\n             llm_text = self._llm_cleanup(cleaned)\r\n             return llm_text or cleaned\r\n@@ -106,13 +110,20 @@\n         Для препроцессинга используем тот же ключ, что и для Whisper через Groq,\r\n         чтобы пользователь вводил его один раз в одном месте.\r\n \r\n         ВАЖНО:\r\n-        - если у GroqPostprocessConfig нет поля api_key, берём его через getattr;\r\n-        - ожидается, что туда заранее прокинут recognition.groq.api_key.\r\n+        - ключ берём из self.config.groq.api_key, который должен быть\r\n+          прокинут из recognition.groq.api_key;\r\n+        - модель LLM берём из recognition.groq.model_process.\r\n         \"\"\"\r\n         api_key = (getattr(self.config.groq, \"api_key\", \"\") or \"\").strip()\r\n-        model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n+        # model_process живёт в RecognitionConfig.groq, но для обратной\r\n+        # совместимости оставляем fallback на postprocess.groq.model.\r\n+        model = (\r\n+            getattr(self.config.groq, \"model_process\", \"\") or\r\n+            self.config.groq.model or\r\n+            \"moonshotai/kimi-k2-instruct\"\r\n+        ).strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n \r\n@@ -171,10 +182,16 @@\n \r\n         return content.strip()\r\n \r\n     def _llm_openai(self, text: str) -> str:\r\n-        api_key = (self.config.openai.api_key or \"\").strip()\r\n-        model = (self.config.openai.model or \"gpt-5.1\").strip()\r\n+        api_key = (getattr(self.config.openai, \"api_key\", \"\") or \"\").strip()\r\n+        # Модель для постобработки берём из recognition.openai.model_process,\r\n+        # с fallback на postprocess.openai.model.\r\n+        model = (\r\n+            getattr(self.config.openai, \"model_process\", \"\") or\r\n+            self.config.openai.model or\r\n+            \"gpt-5.1\"\r\n+        ).strip()\r\n         base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\"OpenAI LLM: API‑ключ не задан.\")\r\n"
                },
                {
                    "date": 1764305979634,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -117,12 +117,14 @@\n         \"\"\"\r\n         api_key = (getattr(self.config.groq, \"api_key\", \"\") or \"\").strip()\r\n         # model_process живёт в RecognitionConfig.groq, но для обратной\r\n         # совместимости оставляем fallback на postprocess.groq.model.\r\n+        # Groq деактивировал moonshotai/kimi-k2-instruct. По умолчанию используем\r\n+        # актуальную модель, если в конфиге не задано иное.\r\n         model = (\r\n             getattr(self.config.groq, \"model_process\", \"\") or\r\n             self.config.groq.model or\r\n-            \"moonshotai/kimi-k2-instruct\"\r\n+            \"llama-3.1-70b-versatile\"\r\n         ).strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n@@ -160,8 +162,15 @@\n         if resp.status_code == 401:\r\n             raise RuntimeError(\"Groq LLM: неверный или отсутствующий API‑ключ (401).\")\r\n         if resp.status_code == 429:\r\n             raise RuntimeError(\"Groq LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n+        if resp.status_code == 400:\r\n+            # Специальная обработка случая, когда модель снята с поддержки.\r\n+            logger.error(\"Groq LLM HTTP 400: {}\", resp.text[:500])\r\n+            raise RuntimeError(\r\n+                \"Groq LLM: выбранная модель больше не поддерживается (400). \"\r\n+                \"Откройте настройки (⚙️) и укажите актуальную модель Groq LLM.\"\r\n+            )\r\n         if not resp.is_success:\r\n             logger.error(\"Groq LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n             raise RuntimeError(f\"Groq LLM: ошибка сервера ({resp.status_code}).\")\r\n \r\n"
                },
                {
                    "date": 1764306409572,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,9 +122,9 @@\n         # актуальную модель, если в конфиге не задано иное.\r\n         model = (\r\n             getattr(self.config.groq, \"model_process\", \"\") or\r\n             self.config.groq.model or\r\n-            \"llama-3.1-70b-versatile\"\r\n+            \"moonshotai/kimi-k2-instruct\"\r\n         ).strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n"
                },
                {
                    "date": 1764467389551,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -106,8 +106,10 @@\n         return text\r\n \r\n     def _llm_groq(self, text: str) -> str:\r\n         \"\"\"\r\n+        Вызов Groq LLM (chat.completions) для постобработки текста.\r\n+\r\n         Для препроцессинга используем тот же ключ, что и для Whisper через Groq,\r\n         чтобы пользователь вводил его один раз в одном месте.\r\n \r\n         ВАЖНО:\r\n@@ -152,13 +154,26 @@\n \r\n         try:\r\n             resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n         except httpx.TimeoutException as exc:\r\n-            logger.exception(\"Groq LLM timeout: {}\", exc)\r\n-            raise RuntimeError(\"Groq LLM: превышено время ожидания ответа.\") from exc\r\n+            # Явно фиксируем, что это ошибка LLM Groq, а не распознавания.\r\n+            logger.error(\r\n+                \"LLM (Groq) timeout, using regex-only postprocess. model={}, error={}\",\r\n+                model,\r\n+                exc,\r\n+            )\r\n+            raise RuntimeError(\r\n+                f\"Groq LLM timeout (model={model}): {exc}\"\r\n+            ) from exc\r\n         except httpx.RequestError as exc:\r\n-            logger.exception(\"Groq LLM network error: {}\", exc)\r\n-            raise RuntimeError(\"Groq LLM: сетевая ошибка при обращении к API.\") from exc\r\n+            logger.error(\r\n+                \"LLM (Groq) network error, using regex-only postprocess. model={}, error={}\",\r\n+                model,\r\n+                exc,\r\n+            )\r\n+            raise RuntimeError(\r\n+                f\"Groq LLM network error (model={model}): {exc}\"\r\n+            ) from exc\r\n \r\n         if resp.status_code == 401:\r\n             raise RuntimeError(\"Groq LLM: неверный или отсутствующий API‑ключ (401).\")\r\n         if resp.status_code == 429:\r\n"
                },
                {
                    "date": 1764469440953,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -206,8 +206,15 @@\n \r\n         return content.strip()\r\n \r\n     def _llm_openai(self, text: str) -> str:\r\n+        \"\"\"\r\n+        Вызов OpenAI LLM для постобработки.\r\n+\r\n+        ВАЖНО:\r\n+        - ключ всегда берём из recognition.openai.api_key, который SettingsDialog пишет в config.yaml;\r\n+        - если ключ пустой — сразу даём понятную ошибку, без сетевых запросов.\r\n+        \"\"\"\r\n         api_key = (getattr(self.config.openai, \"api_key\", \"\") or \"\").strip()\r\n         # Модель для постобработки берём из recognition.openai.model_process,\r\n         # с fallback на postprocess.openai.model.\r\n         model = (\r\n@@ -217,9 +224,12 @@\n         ).strip()\r\n         base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n \r\n         if not api_key:\r\n-            raise RuntimeError(\"OpenAI LLM: API‑ключ не задан.\")\r\n+            raise RuntimeError(\r\n+                \"OpenAI LLM: отсутствует API‑ключ. \"\r\n+                \"Заполните поле 'OpenAI API key' в настройках и сохраните.\"\r\n+            )\r\n \r\n         # Совместимый с OpenAI / proxy формат /chat/completions\r\n         url = base_url.rstrip(\"/\") + \"/chat/completions\"\r\n         headers = {\r\n@@ -273,216 +283,5 @@\n \r\n         if not isinstance(content, str):\r\n             raise RuntimeError(\"OpenAI LLM: контент ответа не является строкой.\")\r\n \r\n-        return content.strip()\n-from __future__ import annotations\r\n-\r\n-import re\r\n-from dataclasses import dataclass\r\n-from typing import Any, Dict\r\n-\r\n-import httpx  # type: ignore[import]\r\n-from loguru import logger  # type: ignore[import]\r\n-\r\n-from config.settings import PostprocessConfig\r\n-\r\n-\r\n-@dataclass\r\n-class TextPostprocessor:\r\n-    \"\"\"\r\n-    Постобработка текста.\r\n-\r\n-    Режимы:\r\n-      - enabled = False  -> только возвращаем текст как есть (без даже regex)\r\n-      - enabled = True, mode = \"simple\" -> лёгкая regex-очистка\r\n-      - enabled = True, mode = \"llm\"    -> regex-очистка + LLM (Groq/OpenAI)\r\n-\r\n-    ВАЖНО: при любой ошибке LLM мы не ломаем UX, а возвращаем regex-вариант.\r\n-    \"\"\"\r\n-\r\n-    config: PostprocessConfig\r\n-\r\n-    def process(self, text: str) -> str:\r\n-        text = text or \"\"\r\n-\r\n-        # Постпроцессинг полностью выключен\r\n-        if not self.config.enabled:\r\n-            return self._simple_cleanup(text)\r\n-\r\n-        # Только простая очистка\r\n-        if (self.config.mode or \"simple\").lower() == \"simple\":\r\n-            return self._simple_cleanup(text)\r\n-\r\n-        # Режим LLM: сначала regex, потом попытка прогнать через модель\r\n-        cleaned = self._simple_cleanup(text)\r\n-        try:\r\n-            llm_text = self._llm_cleanup(cleaned)\r\n-            return llm_text or cleaned\r\n-        except Exception as exc:  # noqa: BLE001\r\n-            logger.exception(\"LLM postprocess failed, fallback to regex-only: {}\", exc)\r\n-            return cleaned\r\n-\r\n-    # ------------------------------------------------------------------ simple regex\r\n-\r\n-    @staticmethod\r\n-    def _simple_cleanup(text: str) -> str:\r\n-        # Удалить лишние пробелы\r\n-        text = re.sub(r\"\\s+\", \" \", text).strip()\r\n-\r\n-        # Пробел перед знаками препинания убрать\r\n-        text = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", text)\r\n-\r\n-        # Пробел после запятой/точки/восклицательного/вопросительного знака\r\n-        text = re.sub(r\"([,.!?;:])([^\\s])\", r\"\\1 \\2\", text)\r\n-\r\n-        # Удалить артефакты вида [BLANK_AUDIO] и т.п.\r\n-        text = re.sub(r\"\\[[^\\]]+\\]\", \"\", text).strip()\r\n-\r\n-        return text\r\n-\r\n-    # ------------------------------------------------------------------ LLM cleanup\r\n-\r\n-    def _llm_cleanup(self, text: str) -> str:\r\n-        \"\"\"\r\n-        Прогоняет текст через LLM (Groq/OpenAI) для улучшения грамматики и пунктуации.\r\n-\r\n-        Промпт:\r\n-        «Исправь опечатки, добавь пунктуацию, сделай предложение грамматически верным.\r\n-         Не меняй смысл. Ответь ТОЛЬКО исправленным текстом.»\r\n-        \"\"\"\r\n-        backend = (self.config.llm_backend or \"groq\").lower()\r\n-\r\n-        if backend == \"groq\":\r\n-            return self._llm_groq(text)\r\n-        if backend == \"openai\":\r\n-            return self._llm_openai(text)\r\n-\r\n-        logger.warning(\"Unknown LLM backend '{}', fallback to original text\", backend)\r\n-        return text\r\n-\r\n-    def _llm_groq(self, text: str) -> str:\r\n-        # Берём ключ в приоритете из groq.api_key (первое место в конфиге/настройках)\r\n-        api_key = (self.config.groq.api_key or \"\").strip()\r\n-        model = (self.config.groq.model or \"moonshotai/kimi-k2-instruct\").strip()\r\n-\r\n-        if not api_key:\r\n-            raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n-\r\n-        url = \"https://api.groq.com/openai/v1/chat/completions\"\r\n-        headers = {\r\n-            \"Authorization\": f\"Bearer {api_key}\",\r\n-            \"Content-Type\": \"application/json\",\r\n-        }\r\n-        payload: Dict[str, Any] = {\r\n-            \"model\": model,\r\n-            \"messages\": [\r\n-                {\r\n-                    \"role\": \"system\",\r\n-                    \"content\": (\r\n-                        \"Ты помощник по русскому языку. \"\r\n-                        \"Исправь опечатки, добавь пунктуацию, сделай текст грамматически верным. \"\r\n-                        \"Не меняй смысл. Ответь ТОЛЬКО исправленным текстом без пояснений.\"\r\n-                    ),\r\n-                },\r\n-                {\"role\": \"user\", \"content\": text},\r\n-            ],\r\n-            \"temperature\": 0.0,\r\n-        }\r\n-\r\n-        try:\r\n-            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n-        except httpx.TimeoutException as exc:\r\n-            logger.exception(\"Groq LLM timeout: {}\", exc)\r\n-            raise RuntimeError(\"Groq LLM: превышено время ожидания ответа.\") from exc\r\n-        except httpx.RequestError as exc:\r\n-            logger.exception(\"Groq LLM network error: {}\", exc)\r\n-            raise RuntimeError(\"Groq LLM: сетевая ошибка при обращении к API.\") from exc\r\n-\r\n-        if resp.status_code == 401:\r\n-            raise RuntimeError(\"Groq LLM: неверный или отсутствующий API‑ключ (401).\")\r\n-        if resp.status_code == 429:\r\n-            raise RuntimeError(\"Groq LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n-        if not resp.is_success:\r\n-            logger.error(\"Groq LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n-            raise RuntimeError(f\"Groq LLM: ошибка сервера ({resp.status_code}).\")\r\n-\r\n-        try:\r\n-            data = resp.json()\r\n-        except ValueError as exc:\r\n-            logger.exception(\"Groq LLM JSON parse error: {}\", exc)\r\n-            raise RuntimeError(\"Groq LLM: не удалось разобрать ответ сервера.\") from exc\r\n-\r\n-        try:\r\n-            content = data[\"choices\"][0][\"message\"][\"content\"]\r\n-        except Exception as exc:  # noqa: BLE001\r\n-            logger.error(\"Groq LLM unexpected response format: {}\", data)\r\n-            raise RuntimeError(\"Groq LLM: неожиданный формат ответа.\") from exc\r\n-\r\n-        if not isinstance(content, str):\r\n-            raise RuntimeError(\"Groq LLM: контент ответа не является строкой.\")\r\n-\r\n-        return content.strip()\r\n-\r\n-    def _llm_openai(self, text: str) -> str:\r\n-        api_key = (self.config.openai.api_key or \"\").strip()\r\n-        model = (self.config.openai.model or \"gpt-5.1\").strip()\r\n-        base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n-\r\n-        if not api_key:\r\n-            raise RuntimeError(\"OpenAI LLM: API‑ключ не задан.\")\r\n-\r\n-        # Совместимый с OpenAI / proxy формат /chat/completions\r\n-        url = base_url.rstrip(\"/\") + \"/chat/completions\"\r\n-        headers = {\r\n-            \"Authorization\": f\"Bearer {api_key}\",\r\n-            \"Content-Type\": \"application/json\",\r\n-        }\r\n-        payload: Dict[str, Any] = {\r\n-            \"model\": model,\r\n-            \"messages\": [\r\n-                {\r\n-                    \"role\": \"system\",\r\n-                    \"content\": (\r\n-                        \"Ты помощник по русскому языку. \"\r\n-                        \"Исправь опечатки, добавь пунктуацию, сделай текст грамматически верным. \"\r\n-                        \"Не меняй смысл. Ответь ТОЛЬКО исправленным текстом без пояснений.\"\r\n-                    ),\r\n-                },\r\n-                {\"role\": \"user\", \"content\": text},\r\n-            ],\r\n-            \"temperature\": 0.0,\r\n-        }\r\n-\r\n-        try:\r\n-            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n-        except httpx.TimeoutException as exc:\r\n-            logger.exception(\"OpenAI LLM timeout: {}\", exc)\r\n-            raise RuntimeError(\"OpenAI LLM: превышено время ожидания ответа.\") from exc\r\n-        except httpx.RequestError as exc:\r\n-            logger.exception(\"OpenAI LLM network error: {}\", exc)\r\n-            raise RuntimeError(\"OpenAI LLM: сетевая ошибка при обращении к API.\") from exc\r\n-\r\n-        if resp.status_code == 401:\r\n-            raise RuntimeError(\"OpenAI LLM: неверный или отсутствующий API‑ключ (401).\")\r\n-        if resp.status_code == 429:\r\n-            raise RuntimeError(\"OpenAI LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n-        if not resp.is_success:\r\n-            logger.error(\"OpenAI LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n-            raise RuntimeError(f\"OpenAI LLM: ошибка сервера ({resp.status_code}).\")\r\n-\r\n-        try:\r\n-            data = resp.json()\r\n-        except ValueError as exc:\r\n-            logger.exception(\"OpenAI LLM JSON parse error: {}\", exc)\r\n-            raise RuntimeError(\"OpenAI LLM: не удалось разобрать ответ сервера.\") from exc\r\n-\r\n-        try:\r\n-            content = data[\"choices\"][0][\"message\"][\"content\"]\r\n-        except Exception as exc:  # noqa: BLE001\r\n-            logger.error(\"OpenAI LLM unexpected response format: {}\", data)\r\n-            raise RuntimeError(\"OpenAI LLM: неожиданный формат ответа.\") from exc\r\n-\r\n-        if not isinstance(content, str):\r\n-            raise RuntimeError(\"OpenAI LLM: контент ответа не является строкой.\")\r\n-\r\n         return content.strip()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1764469530680,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -221,8 +221,10 @@\n             getattr(self.config.openai, \"model_process\", \"\") or\r\n             self.config.openai.model or\r\n             \"gpt-5.1\"\r\n         ).strip()\r\n+        # URL для LLM берём из того же поля, что и для ASR: recognition.openai.base_url\r\n+        # (SettingsDialog пишет его в config.yaml). Если пусто — дефолтный OpenAI endpoint.\r\n         base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\r\n"
                },
                {
                    "date": 1764497425669,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,8 +130,10 @@\n \r\n         if not api_key:\r\n             raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n \r\n+        # Базовый URL для LLM Groq должен быть тем же, что и для транскрибации:\r\n+        # единый публичный endpoint Groq OpenAI-совместимого API.\r\n         url = \"https://api.groq.com/openai/v1/chat/completions\"\r\n         headers = {\r\n             \"Authorization\": f\"Bearer {api_key}\",\r\n             \"Content-Type\": \"application/json\",\r\n@@ -221,10 +223,11 @@\n             getattr(self.config.openai, \"model_process\", \"\") or\r\n             self.config.openai.model or\r\n             \"gpt-5.1\"\r\n         ).strip()\r\n-        # URL для LLM берём из того же поля, что и для ASR: recognition.openai.base_url\r\n-        # (SettingsDialog пишет его в config.yaml). Если пусто — дефолтный OpenAI endpoint.\r\n+        # Базовый URL для LLM OpenAI должен быть тем же, что и для транскрибации:\r\n+        # recognition.openai.base_url (SettingsDialog пишет его в config.yaml).\r\n+        # Если пусто — дефолтный OpenAI endpoint.\r\n         base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\r\n"
                },
                {
                    "date": 1764497552858,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -114,23 +114,22 @@\n \r\n         ВАЖНО:\r\n         - ключ берём из self.config.groq.api_key, который должен быть\r\n           прокинут из recognition.groq.api_key;\r\n-        - модель LLM берём из recognition.groq.model_process.\r\n+        - модель LLM берём ТОЛЬКО из recognition.groq.model_process.\r\n         \"\"\"\r\n         api_key = (getattr(self.config.groq, \"api_key\", \"\") or \"\").strip()\r\n-        # model_process живёт в RecognitionConfig.groq, но для обратной\r\n-        # совместимости оставляем fallback на postprocess.groq.model.\r\n-        # Groq деактивировал moonshotai/kimi-k2-instruct. По умолчанию используем\r\n-        # актуальную модель, если в конфиге не задано иное.\r\n-        model = (\r\n-            getattr(self.config.groq, \"model_process\", \"\") or\r\n-            self.config.groq.model or\r\n-            \"moonshotai/kimi-k2-instruct\"\r\n-        ).strip()\r\n+        # Модель LLM Groq берём только из recognition.groq.model_process.\r\n+        # Никаких fallback'ов на postprocess.groq.model и жёстких дефолтов.\r\n+        model = getattr(self.config.groq, \"model_process\", \"\") or \"\"\r\n+        model = model.strip()\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\"Groq LLM: API‑ключ не задан.\")\r\n+        if not model:\r\n+            raise RuntimeError(\r\n+                \"Groq LLM: модель не задана. Укажите модель в настройках (поле Groq LLM model).\"\r\n+            )\r\n \r\n         # Базовый URL для LLM Groq должен быть тем же, что и для транскрибации:\r\n         # единый публичный endpoint Groq OpenAI-совместимого API.\r\n         url = \"https://api.groq.com/openai/v1/chat/completions\"\r\n"
                },
                {
                    "date": 1764498036054,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -129,8 +129,10 @@\n             raise RuntimeError(\r\n                 \"Groq LLM: модель не задана. Укажите модель в настройках (поле Groq LLM model).\"\r\n             )\r\n \r\n+        logger.info(\"Groq LLM postprocess using model: {}\", model)\r\n+\r\n         # Базовый URL для LLM Groq должен быть тем же, что и для транскрибации:\r\n         # единый публичный endpoint Groq OpenAI-совместимого API.\r\n         url = \"https://api.groq.com/openai/v1/chat/completions\"\r\n         headers = {\r\n"
                },
                {
                    "date": 1764499080278,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -237,8 +237,9 @@\n             )\r\n \r\n         # Совместимый с OpenAI / proxy формат /chat/completions\r\n         url = base_url.rstrip(\"/\") + \"/chat/completions\"\r\n+        logger.info(\"OpenAI LLM postprocess URL: {}\", url)\r\n         headers = {\r\n             \"Authorization\": f\"Bearer {api_key}\",\r\n             \"Content-Type\": \"application/json\",\r\n         }\r\n"
                },
                {
                    "date": 1764499185447,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -224,12 +224,15 @@\n             getattr(self.config.openai, \"model_process\", \"\") or\r\n             self.config.openai.model or\r\n             \"gpt-5.1\"\r\n         ).strip()\r\n-        # Базовый URL для LLM OpenAI должен быть тем же, что и для транскрибации:\r\n-        # recognition.openai.base_url (SettingsDialog пишет его в config.yaml).\r\n-        # Если пусто — дефолтный OpenAI endpoint.\r\n-        base_url = (self.config.openai.base_url or \"https://api.openai.com/v1\").strip()\r\n+        # Базовый URL для LLM OpenAI берём ТОЛЬКО из recognition.openai.base_url.\r\n+        # Никакого дефолтного https://api.openai.com/v1 здесь быть не должно.\r\n+        base_url = (self.config.openai.base_url or \"\").strip()\r\n+        if not base_url:\r\n+            raise RuntimeError(\r\n+                \"OpenAI LLM: base_url не задан. Укажите 'OpenAI Base URL' в настройках.\"\r\n+            )\r\n \r\n         if not api_key:\r\n             raise RuntimeError(\r\n                 \"OpenAI LLM: отсутствует API‑ключ. \"\r\n"
                },
                {
                    "date": 1764499264395,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -241,8 +241,9 @@\n \r\n         # Совместимый с OpenAI / proxy формат /chat/completions\r\n         url = base_url.rstrip(\"/\") + \"/chat/completions\"\r\n         logger.info(\"OpenAI LLM postprocess URL: {}\", url)\r\n+        logger.info(\"OpenAI LLM postprocess using api_key (first 8 chars): {}***\", api_key[:8])\r\n         headers = {\r\n             \"Authorization\": f\"Bearer {api_key}\",\r\n             \"Content-Type\": \"application/json\",\r\n         }\r\n"
                },
                {
                    "date": 1764499973040,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -214,21 +214,23 @@\n         Вызов OpenAI LLM для постобработки.\r\n \r\n         ВАЖНО:\r\n         - ключ всегда берём из recognition.openai.api_key, который SettingsDialog пишет в config.yaml;\r\n-        - если ключ пустой — сразу даём понятную ошибку, без сетевых запросов.\r\n+        - модель берём из recognition.openai.model_process (с fallback на recognition.openai.model);\r\n+        - base_url берём ТОЛЬКО из recognition.openai.base_url;\r\n+        - блок postprocess.openai НЕ содержит ни ключа, ни base_url.\r\n         \"\"\"\r\n+        # Ключ и модель/URL приходят из recognition.openai.*,\r\n+        # которые App и SettingsDialog прокидывают в config.postprocess.openai\r\n+        # как \"прозрачный\" контейнер.\r\n         api_key = (getattr(self.config.openai, \"api_key\", \"\") or \"\").strip()\r\n-        # Модель для постобработки берём из recognition.openai.model_process,\r\n-        # с fallback на postprocess.openai.model.\r\n-        model = (\r\n-            getattr(self.config.openai, \"model_process\", \"\") or\r\n-            self.config.openai.model or\r\n-            \"gpt-5.1\"\r\n-        ).strip()\r\n-        # Базовый URL для LLM OpenAI берём ТОЛЬКО из recognition.openai.base_url.\r\n-        # Никакого дефолтного https://api.openai.com/v1 здесь быть не должно.\r\n-        base_url = (self.config.openai.base_url or \"\").strip()\r\n+        model = (getattr(self.config.openai, \"model_process\", \"\") or \"\").strip()\r\n+        if not model:\r\n+            # Fallback на основную модель OpenAI ASR, если отдельная LLM‑модель не задана.\r\n+            model = (getattr(self.config.openai, \"model\", \"\") or \"\").strip()\r\n+\r\n+        base_url = (getattr(self.config.openai, \"base_url\", \"\") or \"\").strip()\r\n+\r\n         if not base_url:\r\n             raise RuntimeError(\r\n                 \"OpenAI LLM: base_url не задан. Укажите 'OpenAI Base URL' в настройках.\"\r\n             )\r\n@@ -238,11 +240,17 @@\n                 \"OpenAI LLM: отсутствует API‑ключ. \"\r\n                 \"Заполните поле 'OpenAI API key' в настройках и сохраните.\"\r\n             )\r\n \r\n+        if not model:\r\n+            raise RuntimeError(\r\n+                \"OpenAI LLM: модель не задана. Укажите модель в настройках (поле OpenAI postprocess model).\"\r\n+            )\r\n+\r\n         # Совместимый с OpenAI / proxy формат /chat/completions\r\n         url = base_url.rstrip(\"/\") + \"/chat/completions\"\r\n         logger.info(\"OpenAI LLM postprocess URL: {}\", url)\r\n+        logger.info(\"OpenAI LLM postprocess using model: {}\", model)\r\n         logger.info(\"OpenAI LLM postprocess using api_key (first 8 chars): {}***\", api_key[:8])\r\n         headers = {\r\n             \"Authorization\": f\"Bearer {api_key}\",\r\n             \"Content-Type\": \"application/json\",\r\n"
                },
                {
                    "date": 1764591337771,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -6,9 +6,9 @@\n \r\n import httpx  # type: ignore[import]\r\n from loguru import logger  # type: ignore[import]\r\n \r\n-from config.settings import PostprocessConfig\r\n+from src.config.settings import PostprocessConfig\r\n \r\n \r\n @dataclass\r\n class TextPostprocessor:\r\n"
                },
                {
                    "date": 1764671782224,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,8 +172,10 @@\n                 \"LLM (Groq) network error, using regex-only postprocess. model={}, error={}\",\r\n                 model,\r\n                 exc,\r\n             )\r\n+            # Не пробрасываем исключение выше — даём верхнему уровню спокойно\r\n+            # откатиться к regex-only без падения потока/Qt.\r\n             raise RuntimeError(\r\n                 f\"Groq LLM network error (model={model}): {exc}\"\r\n             ) from exc\r\n \r\n"
                },
                {
                    "date": 1764711724940,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -155,11 +155,12 @@\n             \"temperature\": 0.0,\r\n         }\r\n \r\n         try:\r\n-            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n+            # Короткий таймаут 2 секунды, чтобы не подвешивать постпроцессинг.\r\n+            resp = httpx.post(url, headers=headers, json=payload, timeout=2.0)\r\n         except httpx.TimeoutException as exc:\r\n-            # Явно фиксируем, что это ошибка LLM Groq, а не распознавания.\r\n+            # Логируем кратко без полного traceback.\r\n             logger.error(\r\n                 \"LLM (Groq) timeout, using regex-only postprocess. model={}, error={}\",\r\n                 model,\r\n                 exc,\r\n@@ -167,8 +168,9 @@\n             raise RuntimeError(\r\n                 f\"Groq LLM timeout (model={model}): {exc}\"\r\n             ) from exc\r\n         except httpx.RequestError as exc:\r\n+            # Любая другая сетевая ошибка Groq LLM — тоже без гигантского stacktrace.\r\n             logger.error(\r\n                 \"LLM (Groq) network error, using regex-only postprocess. model={}, error={}\",\r\n                 model,\r\n                 exc,\r\n@@ -273,14 +275,17 @@\n             \"temperature\": 0.0,\r\n         }\r\n \r\n         try:\r\n-            resp = httpx.post(url, headers=headers, json=payload, timeout=30.0)\r\n+            # Короткий таймаут 2 секунды, чтобы не подвешивать постпроцессинг.\r\n+            resp = httpx.post(url, headers=headers, json=payload, timeout=2.0)\r\n         except httpx.TimeoutException as exc:\r\n-            logger.exception(\"OpenAI LLM timeout: {}\", exc)\r\n+            # Логируем кратко без полного traceback.\r\n+            logger.error(\"OpenAI LLM timeout: {}\", exc)\r\n             raise RuntimeError(\"OpenAI LLM: превышено время ожидания ответа.\") from exc\r\n         except httpx.RequestError as exc:\r\n-            logger.exception(\"OpenAI LLM network error: {}\", exc)\r\n+            # Любая другая сетевая ошибка OpenAI LLM — тоже без гигантского stacktrace.\r\n+            logger.error(\"OpenAI LLM network error: {}\", exc)\r\n             raise RuntimeError(\"OpenAI LLM: сетевая ошибка при обращении к API.\") from exc\r\n \r\n         if resp.status_code == 401:\r\n             raise RuntimeError(\"OpenAI LLM: неверный или отсутствующий API‑ключ (401).\")\r\n"
                },
                {
                    "date": 1764731399350,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -62,10 +62,17 @@\n \r\n         try:\r\n             llm_text = self._llm_cleanup(cleaned)\r\n             return llm_text or cleaned\r\n+        except RuntimeError as exc:\r\n+            # Эти ошибки уже залогированы в _llm_groq/_llm_openai.\r\n+            # Просто возвращаем очищенный текст.\r\n+            logger.warning(\"LLM postprocess failed, fallback to regex-only: {}\", exc)\r\n+            return cleaned\r\n         except Exception as exc:  # noqa: BLE001\r\n-            logger.exception(\"LLM postprocess failed, fallback to regex-only: {}\", exc)\r\n+            logger.exception(\r\n+                \"Unexpected LLM postprocess error, fallback to regex-only: {}\", exc\r\n+            )\r\n             return cleaned\r\n \r\n     # ------------------------------------------------------------------ simple regex\r\n \r\n@@ -156,46 +163,36 @@\n         }\r\n \r\n         try:\r\n             # Короткий таймаут 2 секунды, чтобы не подвешивать постпроцессинг.\r\n+            # Короткий таймаут 2 секунды, чтобы не подвешивать постпроцессинг.\r\n             resp = httpx.post(url, headers=headers, json=payload, timeout=2.0)\r\n+            resp.raise_for_status()\r\n         except httpx.TimeoutException as exc:\r\n-            # Логируем кратко без полного traceback.\r\n-            logger.error(\r\n-                \"LLM (Groq) timeout, using regex-only postprocess. model={}, error={}\",\r\n-                model,\r\n-                exc,\r\n-            )\r\n-            raise RuntimeError(\r\n-                f\"Groq LLM timeout (model={model}): {exc}\"\r\n-            ) from exc\r\n+            logger.error(\"LLM (Groq) timeout for model {}: {}\", model, exc)\r\n+            raise RuntimeError(f\"Timeout connecting to Groq LLM: {exc}\") from exc\r\n         except httpx.RequestError as exc:\r\n-            # Любая другая сетевая ошибка Groq LLM — тоже без гигантского stacktrace.\r\n+            logger.error(\"LLM (Groq) network error for model {}: {}\", model, exc)\r\n+            raise RuntimeError(f\"Network error connecting to Groq LLM: {exc}\") from exc\r\n+        except httpx.HTTPStatusError as exc:\r\n             logger.error(\r\n-                \"LLM (Groq) network error, using regex-only postprocess. model={}, error={}\",\r\n+                \"LLM (Groq) HTTP error {} for model {}: {}\",\r\n+                exc.response.status_code,\r\n                 model,\r\n-                exc,\r\n+                exc.response.text,\r\n             )\r\n-            # Не пробрасываем исключение выше — даём верхнему уровню спокойно\r\n-            # откатиться к regex-only без падения потока/Qt.\r\n+            if exc.response.status_code == 401:\r\n+                raise RuntimeError(\"Groq LLM: Invalid API key (401).\")\r\n+            if exc.response.status_code == 429:\r\n+                raise RuntimeError(\"Groq LLM: Rate limit exceeded (429).\")\r\n+            if exc.response.status_code == 400:\r\n+                raise RuntimeError(\r\n+                    \"Groq LLM: The selected model may no longer be supported (400).\"\r\n+                )\r\n             raise RuntimeError(\r\n-                f\"Groq LLM network error (model={model}): {exc}\"\r\n+                f\"Groq LLM server error: {exc.response.status_code}\"\r\n             ) from exc\r\n \r\n-        if resp.status_code == 401:\r\n-            raise RuntimeError(\"Groq LLM: неверный или отсутствующий API‑ключ (401).\")\r\n-        if resp.status_code == 429:\r\n-            raise RuntimeError(\"Groq LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n-        if resp.status_code == 400:\r\n-            # Специальная обработка случая, когда модель снята с поддержки.\r\n-            logger.error(\"Groq LLM HTTP 400: {}\", resp.text[:500])\r\n-            raise RuntimeError(\r\n-                \"Groq LLM: выбранная модель больше не поддерживается (400). \"\r\n-                \"Откройте настройки (⚙️) и укажите актуальную модель Groq LLM.\"\r\n-            )\r\n-        if not resp.is_success:\r\n-            logger.error(\"Groq LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n-            raise RuntimeError(f\"Groq LLM: ошибка сервера ({resp.status_code}).\")\r\n \r\n         try:\r\n             data = resp.json()\r\n         except ValueError as exc:\r\n@@ -276,25 +273,32 @@\n         }\r\n \r\n         try:\r\n             # Короткий таймаут 2 секунды, чтобы не подвешивать постпроцессинг.\r\n+            # Короткий таймаут 2 секунды, чтобы не подвешивать постпроцессинг.\r\n             resp = httpx.post(url, headers=headers, json=payload, timeout=2.0)\r\n+            resp.raise_for_status()\r\n         except httpx.TimeoutException as exc:\r\n-            # Логируем кратко без полного traceback.\r\n-            logger.error(\"OpenAI LLM timeout: {}\", exc)\r\n-            raise RuntimeError(\"OpenAI LLM: превышено время ожидания ответа.\") from exc\r\n+            logger.error(\"OpenAI LLM timeout for model {}: {}\", model, exc)\r\n+            raise RuntimeError(f\"Timeout connecting to OpenAI LLM: {exc}\") from exc\r\n         except httpx.RequestError as exc:\r\n-            # Любая другая сетевая ошибка OpenAI LLM — тоже без гигантского stacktrace.\r\n-            logger.error(\"OpenAI LLM network error: {}\", exc)\r\n-            raise RuntimeError(\"OpenAI LLM: сетевая ошибка при обращении к API.\") from exc\r\n+            logger.error(\"OpenAI LLM network error for model {}: {}\", model, exc)\r\n+            raise RuntimeError(f\"Network error connecting to OpenAI LLM: {exc}\") from exc\r\n+        except httpx.HTTPStatusError as exc:\r\n+            logger.error(\r\n+                \"OpenAI LLM HTTP error {} for model {}: {}\",\r\n+                exc.response.status_code,\r\n+                model,\r\n+                exc.response.text,\r\n+            )\r\n+            if exc.response.status_code == 401:\r\n+                raise RuntimeError(\"OpenAI LLM: Invalid API key (401).\")\r\n+            if exc.response.status_code == 429:\r\n+                raise RuntimeError(\"OpenAI LLM: Rate limit exceeded (429).\")\r\n+            raise RuntimeError(\r\n+                f\"OpenAI LLM server error: {exc.response.status_code}\"\r\n+            ) from exc\r\n \r\n-        if resp.status_code == 401:\r\n-            raise RuntimeError(\"OpenAI LLM: неверный или отсутствующий API‑ключ (401).\")\r\n-        if resp.status_code == 429:\r\n-            raise RuntimeError(\"OpenAI LLM: превышен лимит запросов (429). Попробуйте позже.\")\r\n-        if not resp.is_success:\r\n-            logger.error(\"OpenAI LLM HTTP {}: {}\", resp.status_code, resp.text[:500])\r\n-            raise RuntimeError(f\"OpenAI LLM: ошибка сервера ({resp.status_code}).\")\r\n \r\n         try:\r\n             data = resp.json()\r\n         except ValueError as exc:\r\n"
                }
            ],
            "date": 1764291606882,
            "name": "Commit-0",
            "content": "from __future__ import annotations\r\n\r\nimport re\r\nfrom dataclasses import dataclass\r\n\r\nfrom config.settings import PostprocessConfig\r\n\r\n\r\n@dataclass\r\nclass TextPostprocessor:\r\n    \"\"\"\r\n    MVP-реализация постобработки текста.\r\n\r\n    Сейчас:\r\n      - если disabled в конфиге — возвращает текст как есть\r\n      - если mode == \"simple\" или \"llm\" — применяет только простые regex-правки\r\n    Позже сюда можно добавить вызовы LLM (Groq/OpenAI) по полям из PostprocessConfig.\r\n    \"\"\"\r\n\r\n    config: PostprocessConfig\r\n\r\n    def process(self, text: str) -> str:\r\n        if not self.config.enabled:\r\n            return text or \"\"\r\n\r\n        # На первом этапе не различаем simple/llm — делаем только лёгкую очистку\r\n        return self._simple_cleanup(text or \"\")\r\n\r\n    @staticmethod\r\n    def _simple_cleanup(text: str) -> str:\r\n        # Удалить лишние пробелы\r\n        text = re.sub(r\"\\s+\", \" \", text).strip()\r\n\r\n        # Пробел перед знаками препинания убрать\r\n        text = re.sub(r\"\\s+([,.!?;:])\", r\"\\1\", text)\r\n\r\n        # Пробел после запятой/точки/восклицательного/вопросительного знака\r\n        text = re.sub(r\"([,.!?;:])([^\\s])\", r\"\\1 \\2\", text)\r\n\r\n        # Удалить артефакты вида [BLANK_AUDIO] и т.п.\r\n        text = re.sub(r\"\\[[^\\]]+\\]\", \"\", text).strip()\r\n\r\n        return text"
        }
    ]
}